
import numpy as np
from multiprocessing import Pool, shared_memory, Process, Queue
from tqdm import tqdm as tq
import multiprocessing as mp
import random
import queue
import os
import time
from collections import defaultdict
import json

import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection

def incremental_pearson_corr(cumulative, new_power, new_h):
    """
    å¢é‡è®¡ç®— Pearson ç›¸å…³ç³»æ•°
    
    å‚æ•°:
    cumulative - ç´¯ç§¯ç»Ÿè®¡é‡å­—å…¸ï¼ŒåŒ…å«:
        'n': å½“å‰æ ·æœ¬æ•°é‡
        'sum_h': ä¸­é—´å€¼æ€»å’Œ
        'sum_power': åŠŸç‡è½¨è¿¹æ€»å’Œ (å‘é‡)
        'sum_h_sq': ä¸­é—´å€¼å¹³æ–¹å’Œ
        'sum_power_sq': åŠŸç‡è½¨è¿¹å¹³æ–¹å’Œ (å‘é‡)
        'sum_h_power': ä¸­é—´å€¼ä¸åŠŸç‡è½¨è¿¹ä¹˜ç§¯å’Œ (å‘é‡)
        
    new_power - æ–°æ ·æœ¬çš„åŠŸç‡è½¨è¿¹ (å‘é‡)
    new_h - æ–°æ ·æœ¬çš„ä¸­é—´å€¼ (æ ‡é‡)
    
    è¿”å›:
    ç›¸å…³ç³»æ•°å‘é‡
    """
    n = cumulative['n'] + 1
    
    # å¢é‡æ›´æ–°ç´¯ç§¯é‡
    delta_h = new_h - cumulative['sum_h'] / cumulative['n'] if cumulative['n'] > 0 else new_h
    cumulative['sum_h'] += new_h
    cumulative['sum_h_sq'] += new_h**2
    
    delta_power = new_power - cumulative['sum_power'] / cumulative['n'] if cumulative['n'] > 0 else new_power
    cumulative['sum_power'] += new_power
    cumulative['sum_power_sq'] += new_power**2
    
    # æ›´æ–°åæ–¹å·®éƒ¨åˆ†
    if cumulative['n'] > 0:
        cumulative['sum_h_power'] += delta_h * delta_power * cumulative['n'] / n
    
    cumulative['n'] = n
    
    # è®¡ç®—å‡å€¼å’Œæ ‡å‡†å·®
    mean_h = cumulative['sum_h'] / n
    mean_power = cumulative['sum_power'] / n
    
    var_h = cumulative['sum_h_sq'] / n - mean_h**2
    var_power = cumulative['sum_power_sq'] / n - mean_power**2
    
    # é˜²æ­¢åˆ†æ¯ä¸ºé›¶
    var_h = np.maximum(var_h, 1e-10)
    var_power = np.maximum(var_power, 1e-10)
    
    # è®¡ç®—åæ–¹å·®
    cov_h_power = cumulative['sum_h_power'] / n
    
    # è®¡ç®—ç›¸å…³ç³»æ•°
    corr = cov_h_power / np.sqrt(var_h * var_power)
    
    # å¤„ç†æ— æ•ˆå€¼
    corr = np.nan_to_num(corr, nan=0.0, posinf=0.0, neginf=0.0)
    
    return corr

# def incremental_pearson_corr(cumulative, new_power, new_h):
#     """
#     å¢é‡è®¡ç®— Pearson ç›¸å…³ç³»æ•° - ä½¿ç”¨Welfordç®—æ³•
#     """
#     n = cumulative['n'] + 1
    
#     # ä¿å­˜æ—§çš„ç»Ÿè®¡é‡
#     old_mean_h = cumulative['mean_h']
#     old_mean_power = cumulative['mean_power'].copy()
    
#     # æ›´æ–°å‡å€¼
#     cumulative['mean_h'] = old_mean_h + (new_h - old_mean_h) / n
#     cumulative['mean_power'] = old_mean_power + (new_power - old_mean_power) / n
    
#     # æ›´æ–°åæ–¹å·®å’Œæ–¹å·®
#     if n > 1:
#         # æ›´æ–°åæ–¹å·®
#         cumulative['cov_sum'] += (new_h - old_mean_h) * (new_power - cumulative['mean_power'])
        
#         # æ›´æ–°hçš„æ–¹å·®
#         cumulative['var_h_sum'] += (new_h - old_mean_h) * (new_h - cumulative['mean_h'])
        
#         # æ›´æ–°powerçš„æ–¹å·®
#         cumulative['var_power_sum'] += (new_power - old_mean_power) * (new_power - cumulative['mean_power'])
    
#     cumulative['n'] = n
    
#     # å½“n<2æ—¶ï¼Œç›¸å…³ç³»æ•°æœªå®šä¹‰ï¼Œè¿”å›0
#     if n < 2:
#         return np.zeros_like(new_power)
    
#     # è®¡ç®—æ–¹å·®å’Œåæ–¹å·®
#     var_h = cumulative['var_h_sum'] / (n - 1)
#     var_power = cumulative['var_power_sum'] / (n - 1)
#     cov_h_power = cumulative['cov_sum'] / (n - 1)
    
#     # é˜²æ­¢åˆ†æ¯ä¸ºé›¶
#     denominator = np.sqrt(var_h * var_power)
#     denominator[denominator == 0] = np.inf
    
#     # è®¡ç®—ç›¸å…³ç³»æ•°
#     corr = cov_h_power / denominator
    
#     # å¤„ç†æ— æ•ˆå€¼å¹¶é™åˆ¶èŒƒå›´
#     corr = np.nan_to_num(corr, nan=0.0, posinf=0.0, neginf=0.0)
#     corr = np.clip(corr, -1.0, 1.0)
    
#     return corr

def distance(plaintext, key):
    product = (key * 1729) % 3329
    temp = (plaintext + product) % 3329
    hwc = bin(temp).count('1')
    return hwc

def process_key(shared_mem_info, key, stop_trace_num):
    """
    å¤„ç†å•ä¸ªå¯†é’¥çš„æ‰€æœ‰è½¨è¿¹æ•°é‡
    """
    shm_name, shape, dtype = shared_mem_info
    try:
        existing_shm = shared_memory.SharedMemory(name=shm_name)
        power_trace_mat = np.ndarray(shape, dtype=dtype, buffer=existing_shm.buf)
        
        # åˆå§‹åŒ–ç´¯ç§¯ç»Ÿè®¡é‡
        cumulative = {
            'n': 0,
            'sum_h': 0,
            'sum_power': np.zeros(shape[1]),
            'sum_h_sq': 0,
            'sum_power_sq': np.zeros(shape[1]),
            'sum_h_power': np.zeros(shape[1])
        }
        
        correlations = []
        
        for trace_num in range(1, stop_trace_num + 1):
            # ç›´æ¥ä½¿ç”¨æ˜æ–‡ä½œä¸ºè¡Œç´¢å¼•
            plaintext = trace_num - 1
            power_data = power_trace_mat[plaintext]
            h_val = distance(plaintext, key)
            
            # å¢é‡è®¡ç®—ç›¸å…³ç³»æ•°
            corr = incremental_pearson_corr(cumulative, power_data, h_val)
            max_corr = np.max(np.abs(corr))
            correlations.append(max_corr)
            
            # æ¯å¤„ç†100ä¸ªè½¨è¿¹æ•°æ‰“å°ä¸€æ¬¡è¿›åº¦
            if trace_num % 100 == 0 and key % 100 == 0:
                print(f"Key {key} - Trace {trace_num}/{stop_trace_num} - Max Corr: {max_corr:.4f}")
        
        return (key, correlations)
    
    finally:
        if 'existing_shm' in locals():
            existing_shm.close()

class GetCpaTraceNum:
    def __init__(self, power_trace_file, sample_number=5000, plaintext_number=3329, key_number=3329,
                 process_number=None, low_sample=None, high_sample=None):
        self.power_trace_file = power_trace_file
        self.sample_number = sample_number
        self.key_number = key_number
        self.plaintext_number = plaintext_number
        self.process_number = process_number or max(1, mp.cpu_count() - 2)
        
        # è‡ªåŠ¨é€‚é…CPUæ ¸å¿ƒæ•°
        max_processes = min(32, os.cpu_count() * 2)
        self.process_number = min(self.process_number, max_processes)
        
        if low_sample is not None:
            self.low_sample = low_sample
        else:
            self.low_sample = 0
        
        if high_sample is not None:
            self.high_sample = high_sample
        else:
            self.high_sample = sample_number
        
        self.sample_number = self.high_sample - self.low_sample
        self.power_trace_mat = None

    def read_power(self):
        """é«˜æ•ˆè¯»å–åŠŸç‡è½¨è¿¹æ•°æ®"""
        print(f"ğŸ“Š è¯»å–åŠŸç‡è½¨è¿¹æ•°æ® (æ ·æœ¬èŒƒå›´: {self.low_sample}-{self.high_sample})")
        
        # åˆå§‹åŒ–åŠŸç‡çŸ©é˜µ
        self.power_trace_mat = np.zeros((self.plaintext_number, self.sample_number), dtype=np.float32)
        
        # ä½¿ç”¨ç¼“å†²åŒºå‡å°‘å†…å­˜åˆ†é…æ¬¡æ•°
        buffer_size = 1000
        buffer = []
        current_index = 0
        
        with tq(total=self.plaintext_number, desc="è¯»å–åŠŸç‡è½¨è¿¹") as bar:
            with open(self.power_trace_file, 'r') as pf:
                for line in pf:
                    if not line.strip():
                        continue
                    
                    try:
                        parts = line.split(':', 1)
                        if len(parts) < 2:
                            continue
                            
                        plaintext_str, power_trace_str = parts
                        plaintext = int(plaintext_str)
                        
                        # åªå¤„ç†åœ¨èŒƒå›´å†…çš„æ˜æ–‡
                        if 0 <= plaintext < self.plaintext_number:
                            power_trace = np.fromstring(power_trace_str, sep=' ', dtype=np.float32)
                            
                            # åº”ç”¨æ ·æœ¬èŒƒå›´
                            if self.low_sample < self.high_sample:
                                power_trace = power_trace[self.low_sample:self.high_sample]
                            
                            buffer.append((plaintext, power_trace))
                            
                            # ç¼“å†²åŒºæ»¡æ—¶æ‰¹é‡å¤„ç†
                            if len(buffer) >= buffer_size:
                                for p, trace in buffer:
                                    self.power_trace_mat[p] = trace
                                buffer = []
                            
                            bar.update(1)
                            current_index += 1
                            
                            if current_index >= self.plaintext_number:
                                break
                    
                    except Exception as e:
                        print(f"è§£æé”™è¯¯: {line.strip()} - {str(e)}")
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if buffer:
            for p, trace in buffer:
                self.power_trace_mat[p] = trace
        
        print(f"âœ… æˆåŠŸè¯»å– {self.plaintext_number} æ¡åŠŸç‡è½¨è¿¹")

    def correlation_trace_num(self, stop_plaintext_num=None, output_file=None):
        if stop_plaintext_num is None:
            stop_plaintext_num = min(self.plaintext_number, 3000)  # é™åˆ¶æœ€å¤§è½¨è¿¹æ•°
        
        if output_file is None:
            raise ValueError('éœ€è¦æŒ‡å®šè¾“å‡ºæ–‡ä»¶è·¯å¾„')
        
        print(f"âš™ï¸ å¼€å§‹åˆ†æè½¨è¿¹æ•°é‡å’Œå¯†é’¥ç›¸å…³æ€§ (è½¨è¿¹æ•°: 1-{stop_plaintext_num})")
        print(f"ğŸ”‘ å¯†é’¥æ€»æ•°: {self.key_number} | è¿›ç¨‹æ•°: {self.process_number}")
        
        # åˆ›å»ºå…±äº«å†…å­˜å­˜æ”¾èƒ½é‡è¿¹çŸ©é˜µ
        start_time = time.time()
        shm = shared_memory.SharedMemory(create=True, size=self.power_trace_mat.nbytes)
        shared_trace_mat = np.ndarray(
            self.power_trace_mat.shape, 
            dtype=self.power_trace_mat.dtype, 
            buffer=shm.buf
        )
        np.copyto(shared_trace_mat, self.power_trace_mat)
        print(f"ğŸ”— å…±äº«å†…å­˜åˆ›å»ºå®Œæˆ (è€—æ—¶: {time.time()-start_time:.2f}s)")
        
        # å‡†å¤‡è¿›ç¨‹æ± 
        shared_mem_info = (shm.name, self.power_trace_mat.shape, self.power_trace_mat.dtype)
        
        # åˆ†æ‰¹å¤„ç†å¯†é’¥ä»¥å‡å°‘å†…å­˜å‹åŠ›
        chunk_size = min(100, max(10, self.key_number // (self.process_number * 2)))
        key_ranges = []
        for i in range(0, self.key_number, chunk_size):
            end = min(i + chunk_size, self.key_number)
            key_ranges.append((i, end))
        
        print(f"ğŸ“¦ å¯†é’¥åˆ†å—: {len(key_ranges)} å— | æ¯å—å¤§å°: {chunk_size} å¯†é’¥")
        
        # ä½¿ç”¨è¿›ç¨‹æ± å¤„ç†å¯†é’¥
        results_dir = "tmp_results"
        os.makedirs(results_dir, exist_ok=True)
        
        with Pool(processes=self.process_number) as pool:
            results = []
            futures = []
            
            # æäº¤ä»»åŠ¡
            for start_key, end_key in key_ranges:
                keys = list(range(start_key, end_key))
                future = pool.apply_async(
                    process_key_range, 
                    (shared_mem_info, keys, stop_plaintext_num, results_dir)
                )
                futures.append(future)
            
            # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
            with tq(total=len(futures), desc="å¤„ç†å¯†é’¥æ‰¹æ¬¡") as pbar:
                for future in futures:
                    future.get()  # ç­‰å¾…å®Œæˆ
                    pbar.update(1)
        
        # åˆå¹¶éƒ¨åˆ†ç»“æœ
        print("ğŸ”— åˆå¹¶éƒ¨åˆ†ç»“æœ...")
        merge_partial_results(results_dir, output_file, self.key_number)
        
        # æ¸…ç†å…±äº«å†…å­˜
        shm.close()
        shm.unlink()
        
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        for f in os.listdir(results_dir):
            os.remove(os.path.join(results_dir, f))
        os.rmdir(results_dir)
        
        print(f"âœ… åˆ†æå®Œæˆ! ç»“æœä¿å­˜è‡³: {output_file}")
        print(f"â±ï¸ æ€»è€—æ—¶: {time.time()-start_time:.2f}ç§’")
    
    def show_traces(self,trace_file=None,highlight_keys=None,stop_plaintext_number=3329):
        if trace_file is None:
            raise ValueError('éœ€è¦æŒ‡å®šæ–‡ä»¶è·¯å¾„')
        # åˆå§‹åŒ–åŠŸç‡çŸ©é˜µ
        trace_result = np.zeros((self.key_number, stop_plaintext_number), dtype=np.float32)
        # ä½¿ç”¨ç¼“å†²åŒºå‡å°‘å†…å­˜åˆ†é…æ¬¡æ•°
        buffer_size = 1000
        buffer = []
        current_index = 0
        with tq(total=self.key_number, desc="è¯»å–ç›¸å…³ç³»æ•°æ›²çº¿") as bar:
            with open(trace_file, 'r') as pf:
                for line in pf:
                    if not line.strip():
                        continue
                    try:
                        parts = line.split(':', 1)
                        if len(parts) < 2:
                            continue
                        key_str, correlation_str = parts
                        key = int(key_str)
                        
                        # åªå¤„ç†åœ¨èŒƒå›´å†…çš„æ˜æ–‡
                        if 0 <= key < self.key_number:
                            correlations = np.fromstring(correlation_str, sep=',', dtype=np.float32)
                            # åº”ç”¨æ ·æœ¬èŒƒå›´
                            buffer.append((key, correlations))
                            # ç¼“å†²åŒºæ»¡æ—¶æ‰¹é‡å¤„ç†
                            if len(buffer) >= buffer_size:
                                for p, trace in buffer:
                                    trace_result[p] = trace
                                buffer = []
                            bar.update(1)
                            current_index += 1
                            
                            if current_index >= self.key_number:
                                break
                    
                    except Exception as e:
                        print(f"è§£æé”™è¯¯: {line.strip()} - {str(e)}")
        
        # å¤„ç†å‰©ä½™æ•°æ®
        if buffer:
            for p, trace in buffer:
                trace_result[p] = trace
        
        print(f"âœ… æˆåŠŸè¯»å– {self.key_number} æ¡ç›¸å…³ç³»æ•°æ›²çº¿")

        print("ğŸ“Š å‡†å¤‡å¯è§†åŒ–ç»“æœ...")
        high_contrast_colors = [   
            '#FFD700',  # é‡‘é»„è‰²
            '#FF6347',  # ç•ªèŒ„çº¢
            '#FF8C00',  # æ·±æ©™è‰²
            '#FF4500',  # æ©™çº¢è‰²
            '#FF1493',  # æ·±ç²‰è‰²
            '#8B0000',  # æ·±çº¢è‰²
            '#FFA500',  # æ©™è‰²
            '#B22222',  # ç –çº¢è‰²
            '#800000',  # æ —è‰²
            '#FF4500',  # æ©™çº¢è‰²
        ]
        # è·å–æ‰€æœ‰å¯†é’¥çš„ç›¸å…³ç³»æ•°æ•°æ®
        # ä»ç¬¬4ä¸ªæ•°æ®å¼€å§‹
        all_corrs = np.array([trace_result[key,3:] for key in range(self.key_number)])
        print('Data read finish')
        # åˆ›å»ºå›¾å½¢å’Œåæ ‡è½´
        fig = plt.figure(figsize=(14, 8))
        # å¦åˆ™åªåˆ›å»ºå•ä¸ªè§†å›¾
        ax = plt.subplot(1, 1, 1)

        # ç»˜åˆ¶æ‰€æœ‰å¯†é’¥çš„ç›¸å…³ç³»æ•°æ›²çº¿ (é«˜æ€§èƒ½æ–¹å¼)
        # ä½¿ç”¨é€æ˜æµ…è‰²ç»˜åˆ¶æ‰€æœ‰æ›²çº¿
        x = np.arange(3,stop_plaintext_number)
        segments = np.array([np.column_stack([x, y]) for y in all_corrs])
        norm = plt.Normalize(0, len(all_corrs))
        lc = LineCollection(segments, cmap='Greys', norm=norm, alpha=0.1, linewidth=0.5)
        ax.add_collection(lc)

        # è®¾ç½®åæ ‡è½´èŒƒå›´
        ax.set_xlim(3, stop_plaintext_number)
        ax.set_ylim(0, 0.5)  # ç›¸å…³ç³»æ•°èŒƒå›´
        #ax.set_ylim(-0.5, 0.35)  # ç›¸å…³ç³»æ•°èŒƒå›´

        # æ·»åŠ ç½‘æ ¼
        ax.grid(True, linestyle='--', alpha=0.6)

        # æ·»åŠ æ ‡ç­¾
        ax.set_xlabel('Trace number')
        ax.set_ylabel('Correlation')
        # åˆ›å»ºé«˜å¯¹æ¯”åº¦é¢œè‰²åˆ—è¡¨ï¼ˆé¿å…è“è‰²ï¼‰
        
        # çªå‡ºæ˜¾ç¤ºç‰¹å®šå¯†é’¥
        if highlight_keys:
            print(f"highlight key: {highlight_keys}")
            #colors = plt.cm.tab10(np.linspace(0, 1, len(highlight_keys)))
            for i, key in enumerate(highlight_keys):
                corr = trace_result[key].flatten()
                label = f'key {key}'
                ax.plot(corr, color=high_contrast_colors[i%10], linewidth=1, alpha=0.9, label=label)
            # æ·»åŠ å›¾ä¾‹
            ax.legend(loc='upper right')

        # æ·»åŠ æ ‡é¢˜
        title = f'CPA result ({self.key_number} keys)'
        if highlight_keys:
            title += f'\nhighlight key(s): {", ".join(map(str, highlight_keys))}'
        plt.suptitle(title, fontsize=14)
        plt.tight_layout()
        plt.show()


def process_key_range(shared_mem_info, keys, stop_trace_num, results_dir):
    """å¤„ç†ä¸€ç»„å¯†é’¥"""
    shm_name, shape, dtype = shared_mem_info
    try:
        existing_shm = shared_memory.SharedMemory(name=shm_name)
        power_trace_mat = np.ndarray(shape, dtype=dtype, buffer=existing_shm.buf)
        
        results = {}
        for key in keys:
            #åˆå§‹åŒ–ç´¯ç§¯ç»Ÿè®¡é‡
            cumulative = {
                'n': 0,
                'sum_h': 0,
                'sum_power': np.zeros(shape[1]),
                'sum_h_sq': 0,
                'sum_power_sq': np.zeros(shape[1]),
                'sum_h_power': np.zeros(shape[1])
            }

            # cumulative = {
            #         'n': 0,
            #         'mean_h': 0.0,
            #         'mean_power': np.zeros(shape),
            #         'cov_sum': np.zeros(shape),
            #         'var_h_sum': 0.0,
            #         'var_power_sum': np.zeros(shape)
            # }
            
            correlations = []
            
            for trace_num in range(1, stop_trace_num + 1):
                plaintext = trace_num - 1
                power_data = power_trace_mat[plaintext]
                h_val = distance(plaintext, key)
                
                # å¢é‡è®¡ç®—ç›¸å…³ç³»æ•°
                corr = incremental_pearson_corr(cumulative, power_data, h_val)
                max_corr = np.max(np.abs(corr))
                correlations.append(max_corr)
            
            results[key] = correlations
        
        # æ‰¹é‡ä¿å­˜ç»“æœåˆ°ä¸´æ—¶æ–‡ä»¶
        output_file = os.path.join(results_dir, f"partial_{keys[0]}_{keys[-1]}.json")
        with open(output_file, 'w') as f:
            json.dump(results, f)
            
        return True
    
    finally:
        if 'existing_shm' in locals():
            existing_shm.close()

def merge_partial_results(results_dir, output_file, total_keys):
    """åˆå¹¶éƒ¨åˆ†ç»“æœæ–‡ä»¶"""
    all_results = {}
    processed_keys = set()
    
    for filename in os.listdir(results_dir):
        if filename.startswith('partial_'):
            filepath = os.path.join(results_dir, filename)
            with open(filepath, 'r') as f:
                partial_results = json.load(f)
                for key_str, correlations in partial_results.items():
                    key = int(key_str)
                    if key not in processed_keys:
                        all_results[key] = correlations
                        processed_keys.add(key)
    
    # æ£€æŸ¥æ˜¯å¦æ‰€æœ‰å¯†é’¥éƒ½å·²å¤„ç†
    if len(all_results) < total_keys:
        print(f"âš ï¸ è­¦å‘Š: åªæœ‰ {len(all_results)}/{total_keys} ä¸ªå¯†é’¥è¢«å¤„ç†")
    
    # æŒ‰å¯†é’¥æ’åºå¹¶å†™å…¥æœ€ç»ˆç»“æœ
    with open(output_file, 'w') as f:
        for key in sorted(all_results.keys()):
            corr_str = ",".join(f"{c:.4f}" for c in all_results[key])
            f.write(f"{key}:{corr_str}\n")
    
    print(f"ğŸ“ å†™å…¥æœ€ç»ˆç»“æœ: {len(all_results)} ä¸ªå¯†é’¥")


if __name__ == "__main__":
    # power_file = 'data/1234/average/average_cd_loop_32.txt'
    # trace_file = 'result/correlation_trace_num_b1234.txt'
    # power_file = 'data/2773/average/average_loop_25.txt'
    # trace_file = 'result/correlation_trace_num_b2773.txt'
    # power_file = 'data/666/average/average_cd_loop_20.txt'
    # trace_file = 'result/correlation_trace_num_b1234.txt'
    # power_file = 'data/2619/average/average_cd_loop_5.txt'
    # trace_file = 'result/correlation_trace_num_b2773.txt'
    # power_file = 'data/1/average/average_cd_loop_5.txt'
    # trace_file = 'result/correlation_trace_num_b1.txt'
    power_file = 'data/2619/average/average_cd_loop_25.txt'
    #trace_file = 'result/correlation_trace_num_b2619_loop25.txt'
    trace_file = 'result/correlation_trace_num_b2773.txt'
    mode = 1 # 0 calculate correlation;1 show;
    cpa = GetCpaTraceNum(
        power_trace_file=power_file,
        sample_number=5000,
        plaintext_number=3329,
        key_number=3329,
        process_number=16,  # å‡å°‘è¿›ç¨‹æ•°ä»¥é¿å…èµ„æºç«äº‰
        low_sample=4300,
        high_sample=5000
    )
    if mode ==0 :
        cpa.read_power()
        cpa.correlation_trace_num(
            stop_plaintext_num=3329,  # å‡å°‘è½¨è¿¹æ•°é‡ä»¥åŠ å¿«è®¡ç®—
            output_file=trace_file
        )
    elif mode == 1:
        cpa.show_traces(trace_file= trace_file,
        highlight_keys=[2773,556],
        # highlight_keys=[1234,2095],
        # highlight_keys=[2619,710],
        # highlight_keys=[666,2663],
        # highlight_keys=[2773,556],
        # highlight_keys=[1,3328],
        stop_plaintext_number=3329)