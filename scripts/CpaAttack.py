
import numpy as np
from multiprocessing import Pool,shared_memory
from tqdm import tqdm as tq
import multiprocessing as mp
import matplotlib.pyplot as plt
from matplotlib.collections import LineCollection
import json
import random

def generate_unique_random_numbers(n,n_max=3329):

    if n < 0 or n >= n_max:
        raise ValueError(f"nÂøÖÈ°ªÂú®0Âà∞{n_max-1}‰πãÈó¥")
    # ÁîüÊàê0Âà∞3328ÁöÑÊï¥Êï∞Â∫èÂàó
    population = list(range(0, 3329))
    
    # ÈöèÊú∫ÊäΩÂèñn‰∏™‰∏çÈáçÂ§çÁöÑÊï∞Â≠ó
    return random.sample(population, n)

def calculate_correlation(x,y):
    """
    pearson correlation
    :param x:
    :param y:
    :return: r
    """
    # ËÆ°ÁÆóÂùáÂÄº
    mean_x = np.mean(x)
    mean_y = np.mean(y)

    # ËÆ°ÁÆóÂàÜÂ≠ê
    numerator = np.sum((x - mean_x) * (y - mean_y))

    # ËÆ°ÁÆóÂàÜÊØç
    denominator = np.sqrt(np.sum((x - mean_x) ** 2)) * np.sqrt(np.sum((y - mean_y) ** 2))

    # ÈÅøÂÖçÂàÜÊØç‰∏∫Èõ∂
    if denominator == 0:
        return 0
    return numerator / denominator

def column_pearson_corr(matrix1, matrix2):
    """
    ËÆ°ÁÆó‰∏§‰∏™Áü©ÈòµÁöÑÂàóÈó¥ Pearson Áõ∏ÂÖ≥Á≥ªÊï∞

    ÂèÇÊï∞:
    matrix1, matrix2 -- Áõ∏ÂêåÂΩ¢Áä∂ÁöÑ‰∫åÁª¥ numpy Êï∞ÁªÑ (m√ón)

    ËøîÂõû:
    Áõ∏ÂÖ≥Á≥ªÊï∞Áü©Èòµ -- ÂΩ¢Áä∂‰∏∫ (1, n) ÁöÑ numpy Êï∞ÁªÑ
    """
    # Á°Æ‰øùÁü©ÈòµÂΩ¢Áä∂Áõ∏Âêå
    assert matrix1.shape == matrix2.shape, "Áü©ÈòµÂΩ¢Áä∂ÂøÖÈ°ªÁõ∏Âêå"

    # ‰∏≠ÂøÉÂåñÁü©Èòµ
    center1 = matrix1 - np.mean(matrix1, axis=0, keepdims=True)
    center2 = matrix2 - np.mean(matrix2, axis=0, keepdims=True)

    # ËÆ°ÁÆóÂàÜÂ≠ê (ÂçèÊñπÂ∑ÆÊ±ÇÂíå)
    numerator = np.sum(center1 * center2, axis=0)

    # ËÆ°ÁÆóÂàÜÊØç (Ê†áÂáÜÂ∑Æ‰πòÁßØ)
    denominator = np.sqrt(np.sum(center1 ** 2, axis=0)) * np.sqrt(np.sum(center2 ** 2, axis=0))

    # Â§ÑÁêÜÂàÜÊØç‰∏∫Èõ∂ÁöÑÊÉÖÂÜµ (ËÆæ‰∏∫0ÈÅøÂÖçNaN)
    denominator[denominator == 0] = np.inf

    # ËÆ°ÁÆóÁõ∏ÂÖ≥Á≥ªÊï∞
    corr = numerator / denominator

    # ËøîÂõûË°åÂêëÈáè (1√ón)
    return corr.reshape(1, -1)

def distance(plaintext,key):
    product = key * 1729%3329
    if plaintext + product > 3329:
        temp = plaintext + product - 3329
    else:
        temp = plaintext + product
    hwc= bin(temp).count('1')
    if plaintext>product:
        temp2 = plaintext-product
    else :
        temp2 = 3329 - product + plaintext
    hwd= bin(temp2).count('1')

    hwproduct = bin(product).count('1')
    hwa = bin(plaintext).count('1')
    
    # return hwc
    # return  hwc + hwd
    # return  0.45*hwc + 0.55*hwd 
    return 2*hwa + hwc + hwd + hwproduct




def process_key_wrapper(args):
    """ÂåÖË£ÖÂáΩÊï∞ÔºåÁî®‰∫éÂ§ÑÁêÜÂçï‰∏™ÂØÜÈí•"""
    key, power_trace_mat, plaintext_list = args
    return process_key(key, power_trace_mat, plaintext_list)


def process_key(key, power_trace_mat, plaintext_list):
    """Â§ÑÁêÜÂçï‰∏™ÂØÜÈí•ÁöÑÂáΩÊï∞ÔºàÁã¨Á´ã‰∫éÁ±ªÔºâ"""
    sample_number = power_trace_mat.shape[1]
    plaintext_mat = np.zeros((len(plaintext_list), sample_number))

    for index, plaintext in enumerate(plaintext_list):
        h = distance(plaintext, key)
        plaintext_mat[index, :] = h

    return key, column_pearson_corr(power_trace_mat, plaintext_mat)


class CPA:
    def __init__(self, power_trace_file,base_file=None, sample_number=5000, plaintext_number=3329, key_number=3329,
                 process_number=None,
                 low_sample = None,
                 high_sample = None):
        self.power_trace_file = power_trace_file
        self.sample_number = sample_number
        self.key_number = key_number
        self.plaintext_number = plaintext_number
        self.process_number = process_number or max(1, mp.cpu_count() - 1)

        if low_sample is not None:
            self.low_sample = low_sample
        else:
            self.low_sample = 0
        
        if high_sample is not None:
            self.high_sample = high_sample
        else :
            self.high_sample = sample_number
        
        self.sample_number = self.high_sample - self.low_sample

        self.plaintext_list = []
        self.power_trace_mat = None
        self.base_power = None
        if base_file is not None:
            with open(base_file,'r') as bf:
                base_power_str = bf.readline()
                self.base_power = np.array(base_power_str.strip().split(', ')).astype(np.float64)
        

    def read_power(self):
        """ËØªÂèñÂäüËÄóËΩ®ËøπÊï∞ÊçÆ"""
        self.power_trace_mat = np.zeros((self.plaintext_number, self.sample_number))

        with tq(total=self.plaintext_number, desc="üìä Reading Power traces") as read_bar:
            with open(self.power_trace_file, 'r') as pf:
                number = 0
                for line in pf:
                    if number >= self.plaintext_number or not line.strip():
                        break
                    try:
                        plaintext_str, power_trace_str = line.split(':', 1)
                        plaintext = int(plaintext_str)
                        power_trace = np.array(power_trace_str.strip().split()).astype(np.float64)
                        #power_trace = np.array([p if p > 0 else -p for p in power_trace])
                        power_trace = power_trace[self.low_sample:self.high_sample]
                        #self.power_trace_mat[number, :] = power_trace
                        self.power_trace_mat[plaintext, :] = power_trace
                        self.plaintext_list.append(plaintext)
                        number += 1
                        read_bar.update(1)
                    except Exception as e:
                        print(f"Error parsing line: {line.strip()} - {str(e)}")

        # Á°Æ‰øùÊï∞ÁªÑÂ§ßÂ∞èÊ≠£Á°Æ
        if number < self.plaintext_number:
            self.power_trace_mat = self.power_trace_mat[:number, :]
            self.plaintext_number = number

        print(f"Successfully read {len(self.plaintext_list)} power traces")

    def analyze(self,output_file=None):
        """Âπ∂Ë°åÂàÜÊûêÊâÄÊúâÂØÜÈí•"""
        print(f"üöÄ Starting parallel CPA analysis with {self.process_number} processes...")

        # ÂáÜÂ§á‰ªªÂä°ÂèÇÊï∞
        tasks = [(key, self.power_trace_mat, self.plaintext_list)
                 for key in range(self.key_number)]

        self.result = {}

        # ‰ΩøÁî®ËøõÁ®ãÊ±†Âπ∂Ë°åÂ§ÑÁêÜ
        with Pool(processes=self.process_number) as pool:
            # ‰ΩøÁî®imap_unorderedËé∑ÂèñÁªìÊûúÔºàÊó†Â∫è‰ΩÜÊõ¥Âø´Ôºâ
            with tq(total=self.key_number, desc="üîë Analyzing keys") as pbar:
                for key, corr in pool.imap_unordered(process_key_wrapper, tasks, chunksize=10):
                    self.result[key] = corr
                    pbar.update(1)

                    # ÊØèÂ§ÑÁêÜ100‰∏™ÂØÜÈí•Êõ¥Êñ∞‰∏ÄÊ¨°ËøõÂ∫¶
                    if pbar.n % 100 == 0:
                        pbar.set_postfix(processed=f"{pbar.n}/{self.key_number}")
        if output_file:
            with open(output_file,'w') as of:
                json.dump(self.result, of, ensure_ascii=False, indent=4,
                default=lambda x: x.tolist() if isinstance(x, np.ndarray) else x.item() if isinstance(x, np.generic) else TypeError) 
        print('‚úÖ CPA analysis completed successfully!')
        return self.result

    def draw_result(self, highlight_keys=None, zoom_range=None, save_path=None, result_file=None,show_max=False):
        """
        ÂèØËßÜÂåñ CPA ÂàÜÊûêÁªìÊûú

        ÂèÇÊï∞:
        highlight_keys: ÈúÄË¶ÅÁ™ÅÂá∫ÊòæÁ§∫ÁöÑÂØÜÈí•ÂàóË°®
        zoom_range: Ë¶ÅÊîæÂ§ßÁöÑÊ†∑Êú¨ËåÉÂõ¥ (start, end)
        save_path: ÂõæÂÉè‰øùÂ≠òË∑ØÂæÑ
        """
        if (not hasattr(self, 'result') or not self.result) and not result_file:
            print("‚ö†Ô∏è ËØ∑ÂÖàËøêË°å analyze() ÊñπÊ≥ïËé∑ÂèñÁªìÊûú")
            return

        print("üìä ÂáÜÂ§áÂèØËßÜÂåñÁªìÊûú...")
        high_contrast_colors = [   
            '#FFD700',  # ÈáëÈªÑËâ≤
            '#FF6347',  # Áï™ËåÑÁ∫¢
            '#FF8C00',  # Ê∑±Ê©ôËâ≤
            '#FF4500',  # Ê©ôÁ∫¢Ëâ≤
            '#FF1493',  # Ê∑±Á≤âËâ≤
            '#8B0000',  # Ê∑±Á∫¢Ëâ≤
            '#FFA500',  # Ê©ôËâ≤
            '#B22222',  # Á†ñÁ∫¢Ëâ≤
            '#800000',  # Ê†óËâ≤
            '#FF4500',  # Ê©ôÁ∫¢Ëâ≤
        ]
        # Ëé∑ÂèñÊâÄÊúâÂØÜÈí•ÁöÑÁõ∏ÂÖ≥Á≥ªÊï∞Êï∞ÊçÆ
        if result_file:
            with open(result_file,'r') as f:
                result = json.load(f)
            all_corrs = np.array([np.array(result[str(key)]).flatten() for key in range(self.key_number)])
        else :
            all_corrs = np.array([self.result[key].flatten() for key in range(self.key_number)])
            #all_corrs = np.array([self.result[key].flatten() for key in range(1,self.key_number)])
        print('Data read finish')
        # ÂàõÂª∫ÂõæÂΩ¢ÂíåÂùêÊ†áËΩ¥
        fig = plt.figure(figsize=(14, 8))
        
        index_max = np.argmax(np.abs(all_corrs))
        max_key = index_max//self.sample_number
        max_index = index_max - (index_max//self.sample_number)*self.sample_number
        print(f'max r {np.max(np.abs(all_corrs))},arg {index_max},-> key:{max_key}, index:{max_index}')
        key_max = index_max//self.sample_number
        if zoom_range:
            # Â¶ÇÊûúÊúâÁº©ÊîæËåÉÂõ¥ÔºåÂàõÂª∫‰∏§‰∏™Â≠êÂõæÔºöÂÖ®Â±ÄËßÜÂõæÂíåÊîæÂ§ßËßÜÂõæ
            ax1 = plt.subplot(2, 1, 1)  # ÂÖ®Â±ÄËßÜÂõæ
            ax2 = plt.subplot(2, 1, 2)  # ÊîæÂ§ßËßÜÂõæ
            axes = (ax1, ax2)
        else:
            # Âê¶ÂàôÂè™ÂàõÂª∫Âçï‰∏™ËßÜÂõæ
            ax = plt.subplot(1, 1, 1)
            axes = (ax,)

        # ÁªòÂà∂ÊâÄÊúâÂØÜÈí•ÁöÑÁõ∏ÂÖ≥Á≥ªÊï∞Êõ≤Á∫ø (È´òÊÄßËÉΩÊñπÂºè)
        for ax in axes:
            # ‰ΩøÁî®ÈÄèÊòéÊµÖËâ≤ÁªòÂà∂ÊâÄÊúâÊõ≤Á∫ø
            x = np.arange(self.sample_number)
            segments = np.array([np.column_stack([x, y]) for y in all_corrs])
            norm = plt.Normalize(0, len(all_corrs))
            lc = LineCollection(segments, cmap='Greys', norm=norm, alpha=0.1, linewidth=0.3)
            ax.add_collection(lc)

            # ËÆæÁΩÆÂùêÊ†áËΩ¥ËåÉÂõ¥
            ax.set_xlim(0, self.sample_number)
            ax.set_ylim(-1, 1)  # Áõ∏ÂÖ≥Á≥ªÊï∞ËåÉÂõ¥
            #ax.set_ylim(-0.5, 0.35)  # Áõ∏ÂÖ≥Á≥ªÊï∞ËåÉÂõ¥

            # Ê∑ªÂä†ÁΩëÊ†º
            ax.grid(True, linestyle='--', alpha=0.6)

            # Ê∑ªÂä†Ê†áÁ≠æ
            ax.set_xlabel('samples index')
            ax.set_ylabel('correlation')
        # ÂàõÂª∫È´òÂØπÊØîÂ∫¶È¢úËâ≤ÂàóË°®ÔºàÈÅøÂÖçËìùËâ≤Ôºâ
        
        # Á™ÅÂá∫ÊòæÁ§∫ÁâπÂÆöÂØÜÈí•
        if highlight_keys:
            print(f"highlight key: {highlight_keys}")
            #colors = plt.cm.tab10(np.linspace(0, 1, len(highlight_keys)))
            
            for ax in axes:
                for i, key in enumerate(highlight_keys):
                    if result_file:
                        corr = np.array(result[str(key)]).flatten()
                    else:
                        corr = self.result[key].flatten()
                    label = f'key {key}'
                    ax.plot(corr, color=high_contrast_colors[i%10], linewidth=2, alpha=0.9, label=label)
                if result_file:
                    corr_max = np.array(result[str(key_max)]).flatten()
                else:
                    corr_max = self.result[key_max].flatten()
                if show_max:
                    label_max = f'key max {key_max}' 
                    ax.plot(corr_max, color=high_contrast_colors[9], linewidth=2, alpha=0.9, label=label_max)
                # Ê∑ªÂä†Âõæ‰æã
                ax.legend(loc='upper right')

        # ËÆæÁΩÆÁº©ÊîæËßÜÂõæËåÉÂõ¥
        if zoom_range:
            ax2.set_title(f'zoom in ({zoom_range[0]}-{zoom_range[1]} samples)')
            ax2.set_xlim(zoom_range)

            # Âú®ÂÖ®Â±ÄËßÜÂõæ‰∏≠Ê†áËÆ∞Áº©ÊîæÂå∫Âüü
            ax1.axvspan(zoom_range[0], zoom_range[1], color='yellow', alpha=0.2)
            ax1.text(zoom_range[0], 0.9, 'zoom in region', fontsize=10,
                    bbox=dict(facecolor='yellow', alpha=0.5))

        # Ê∑ªÂä†Ê†áÈ¢ò
        title = f'CPA result ({self.key_number} keys, {self.sample_number} samples)'
        if highlight_keys:
            title += f'\nhighlight key(s): {", ".join(map(str, highlight_keys))}'
        plt.suptitle(title, fontsize=14)

        plt.tight_layout()

        # ‰øùÂ≠òÊàñÊòæÁ§∫
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            print(f"‚úÖ ÁªìÊûúÂ∑≤‰øùÂ≠òËá≥: {save_path}")
        else:
            plt.show()

    def draw_trace(self,trace_number=0):
        x = np.arange(self.sample_number)
        #plt.plot(x,self.power_trace_mat[trace_number,:]-self.base_power)
        
        ax1 = plt.subplot(2, 1, 1)
        ax2 = plt.subplot(2, 1, 2)
        # ËÆæÁΩÆÂùêÊ†áËΩ¥ËåÉÂõ¥
        ax1.set_xlim(0, self.sample_number)
        ax1.set_ylim(-27000, 27000)  # ËåÉÂõ¥
        ax2.set_xlim(0, self.sample_number)
        ax2.set_ylim(-27000, 27000)  # ËåÉÂõ¥

        # Ê∑ªÂä†ÁΩëÊ†º
        ax1.grid(True, linestyle='--', alpha=0.6)
        ax2.grid(True, linestyle='--', alpha=0.6)

        # Ê∑ªÂä†Ê†áÁ≠æ
        ax1.set_xlabel('samples index')
        ax1.set_ylabel('correlation')
        ax2.set_xlabel('samples index')
        ax2.set_ylabel('correlation')
        label_base = 'base_power_trace'
        ax1.plot(self.base_power, color='#000FFD', linewidth=2, alpha=0.9, label=label_base)
        label_trace = f'power_trace{trace_number}'
        ax2.plot(self.power_trace_mat[trace_number,:], color='#00F00F',  linewidth=2, alpha=0.9, label=label_trace)
        
        ax1.legend(loc='upper right')
        ax2.legend(loc='upper right')
        #plt.plot(x,self.base_power)
        plt.show()

    def analyze_one_process(self,output_file=None):
        self.result = {}
        plaintext_mat = np.zeros((self.power_trace_mat.shape))
        with tq(total=self.key_number, desc="üîë Analyzing keys") as pbar:
            for key in range(self.key_number):
                for index, plaintext in enumerate(self.plaintext_list):
                    h = distance(plaintext, key)
                    plaintext_mat[index, :] = h
                self.result[key] = column_pearson_corr(plaintext_mat,self.power_trace_mat)
                pbar.update(1)
                # ÊØèÂ§ÑÁêÜ100‰∏™ÂØÜÈí•Êõ¥Êñ∞‰∏ÄÊ¨°ËøõÂ∫¶
                if pbar.n % 100 == 0:
                    pbar.set_postfix(processed=f"{pbar.n}/{self.key_number}")
        if output_file:
            with open(output_file,'w') as of:
                json.dump(self.result, of, ensure_ascii=False, indent=4,
                default=lambda x: x.tolist() if isinstance(x, np.ndarray) else x.item() if isinstance(x, np.generic) else TypeError) 
        print('‚úÖ CPA analysis completed successfully!')
        return self.result

    def analyze_one_key(self,output_file=None):
        plaintext_mat = np.zeros((self.power_trace_mat.shape))
        for index,plaintext in enumerate(self.plaintext_list):
            #h = bin(plaintext).count('1')
            if plaintext > 0:
                h = bin(plaintext^(plaintext-1)).count('1')
            else:
                h= bin(plaintext).count('1')
            plaintext_mat[index,:] = h
            if index % 300 == 0:
                print(f'Processed {index} traces')
        correlation = column_pearson_corr(plaintext_mat,self.power_trace_mat)
        t = np.arange(self.sample_number)
        plt.plot(t,correlation[0])
        plt.show()
    
    

        


if __name__ == "__main__":
    #power_file = "data/666/delta/aver_down20_delta.txt"
    #power_file = "data/average/delta_traces_loop_5_max.txt"
    #power_file = 'data/2773/average/average_loop_25_freq.txt'
    # power_file = 'data/2773/average/average_loop_25_align.txt'
    #power_file = 'data/2773/average/average_loop_25_log.txt'
    #power_file = 'data/BeforeData/ntt_pipeline_traces_cd.txt'
    # power_file = 'data/2773/average/average_loop_25.txt'
    #power_file = 'data/2773/average/cd_loop0_mean10.txt'
    # power_file = 'data/2773/average/cd_loop0_mean100.txt'
    #power_file = 'data/2773/delta/mean10_loop0_sub_base.txt'
    #power_file = 'data/mod_1ntt/666/average/average_cd_loop_2.txt'
    power_file = 'data/LNA7m/666/average/averaged-20-lnax7.txt'
    # power_file = 'data/666/source_cd_file/ntt_pipeline_traces-loop2.txt'
    #power_file = 'data/666/average/average_cd_loop_20.txt'
    #power_file = 'data/1234/average/average_cd_loop_32.txt'
    #power_file = 'data/2619/average/average_cd_loop_25.txt'
    #power_file = 'data/2773/source_cd_file/ntt_pipeline_traces-loop0.txt'
    #result_file = 'result/r1.txt'
    result_file = 'result/r2.txt'
    base_file = 'data/BeforeData/base_average.txt'
    s_num=1
    low_sample = 4300
    high_sample = 5000
    mode =2 # 1 analyze ;2 analyze and show result ;3 show power trace ; 4 show one key;5 show corelation by trace number ;else show result file
    cpa = CPA(
        power_trace_file=power_file,
        #base_file = base_file,
        sample_number=5000//s_num,
        plaintext_number=3329,
        key_number=3329,
        process_number=32,
        low_sample=low_sample,
        high_sample=high_sample
    )
    if mode == 1:
        cpa.read_power()
        result = cpa.analyze(
            output_file=result_file,
            )
    elif mode == 2:
        cpa.read_power()
        result = cpa.analyze()
        #result = cpa.analyze_one_process()
        cpa.draw_result(
            # highlight_keys=[2773],
            highlight_keys=[666],
            # highlight_keys=[1234],
            # highlight_keys=[2095],
            # highlight_keys=[2619],
            #highlight_keys=[2663],
            show_max = True
            #zoom_range=(100, 4000),
            #save_path='picture/cpa_result_01.png'
        )
    elif mode == 3 :
        cpa.read_power()
        cpa.draw_trace(112)
    elif mode == 4:
        cpa.read_power()
        cpa.analyze_one_key()
    else :
        cpa.draw_result(
            highlight_keys=[2773],
            #zoom_range=(100, 4000),
            save_path='picture/cpa_result_b_01.png',
            result_file=result_file,
            show_max = True
        )